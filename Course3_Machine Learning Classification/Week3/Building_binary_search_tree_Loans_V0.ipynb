{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing binary decision trees\n",
    "\n",
    "The goal of this notebook is to implement your own binary decision tree classifier. You will:\n",
    "    \n",
    "* Transform categorical variables into binary variables.\n",
    "* Write a function to compute the number of misclassified examples in an intermediate node.\n",
    "* Write a function to find the best feature to split on.\n",
    "* Build a binary decision tree from scratch.\n",
    "* Make predictions using the decision tree.\n",
    "* Evaluate the accuracy of the decision tree.\n",
    "* Visualize the decision at the root node.\n",
    "\n",
    "**Important Note**: In this assignment, we will focus on building decision trees where the data contain **only binary (0 or 1) features**. This allows us to avoid dealing with:\n",
    "* Multiple intermediate nodes in a split\n",
    "* The thresholding issues of real-valued features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the lending club dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2698: DtypeWarning: Columns (19,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "loans = pd.read_csv('lending-club-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>...</th>\n",
       "      <th>sub_grade_num</th>\n",
       "      <th>delinq_2yrs_zero</th>\n",
       "      <th>pub_rec_zero</th>\n",
       "      <th>collections_12_mths_zero</th>\n",
       "      <th>short_emp</th>\n",
       "      <th>payment_inc_ratio</th>\n",
       "      <th>final_d</th>\n",
       "      <th>last_delinq_none</th>\n",
       "      <th>last_record_none</th>\n",
       "      <th>last_major_derog_none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1077501</td>\n",
       "      <td>1296599</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>4975</td>\n",
       "      <td>36 months</td>\n",
       "      <td>10.65</td>\n",
       "      <td>162.87</td>\n",
       "      <td>B</td>\n",
       "      <td>B2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.14350</td>\n",
       "      <td>20141201T000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1077430</td>\n",
       "      <td>1314167</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>60 months</td>\n",
       "      <td>15.27</td>\n",
       "      <td>59.83</td>\n",
       "      <td>C</td>\n",
       "      <td>C4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.39320</td>\n",
       "      <td>20161201T000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1077175</td>\n",
       "      <td>1313524</td>\n",
       "      <td>2400</td>\n",
       "      <td>2400</td>\n",
       "      <td>2400</td>\n",
       "      <td>36 months</td>\n",
       "      <td>15.96</td>\n",
       "      <td>84.33</td>\n",
       "      <td>C</td>\n",
       "      <td>C5</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.25955</td>\n",
       "      <td>20141201T000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1076863</td>\n",
       "      <td>1277178</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>36 months</td>\n",
       "      <td>13.49</td>\n",
       "      <td>339.31</td>\n",
       "      <td>C</td>\n",
       "      <td>C1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.27585</td>\n",
       "      <td>20141201T000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1075269</td>\n",
       "      <td>1311441</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>36 months</td>\n",
       "      <td>7.90</td>\n",
       "      <td>156.46</td>\n",
       "      <td>A</td>\n",
       "      <td>A4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.21533</td>\n",
       "      <td>20141201T000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  member_id  loan_amnt  funded_amnt  funded_amnt_inv        term  \\\n",
       "0  1077501    1296599       5000         5000             4975   36 months   \n",
       "1  1077430    1314167       2500         2500             2500   60 months   \n",
       "2  1077175    1313524       2400         2400             2400   36 months   \n",
       "3  1076863    1277178      10000        10000            10000   36 months   \n",
       "4  1075269    1311441       5000         5000             5000   36 months   \n",
       "\n",
       "   int_rate  installment grade sub_grade          ...          sub_grade_num  \\\n",
       "0     10.65       162.87     B        B2          ...                    0.4   \n",
       "1     15.27        59.83     C        C4          ...                    0.8   \n",
       "2     15.96        84.33     C        C5          ...                    1.0   \n",
       "3     13.49       339.31     C        C1          ...                    0.2   \n",
       "4      7.90       156.46     A        A4          ...                    0.8   \n",
       "\n",
       "  delinq_2yrs_zero pub_rec_zero  collections_12_mths_zero short_emp  \\\n",
       "0              1.0          1.0                       1.0         0   \n",
       "1              1.0          1.0                       1.0         1   \n",
       "2              1.0          1.0                       1.0         0   \n",
       "3              1.0          1.0                       1.0         0   \n",
       "4              1.0          1.0                       1.0         0   \n",
       "\n",
       "  payment_inc_ratio          final_d last_delinq_none last_record_none  \\\n",
       "0           8.14350  20141201T000000                1                1   \n",
       "1           2.39320  20161201T000000                1                1   \n",
       "2           8.25955  20141201T000000                1                1   \n",
       "3           8.27585  20141201T000000                0                1   \n",
       "4           5.21533  20141201T000000                1                1   \n",
       "\n",
       "  last_major_derog_none  \n",
       "0                     1  \n",
       "1                     1  \n",
       "2                     1  \n",
       "3                     1  \n",
       "4                     1  \n",
       "\n",
       "[5 rows x 68 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122607, 68)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans['safe_loans'] = loans['bad_loans'].apply(lambda x : +1 if x==0 else -1)\n",
    "loans = loans.drop(['bad_loans'], axis = 1)\n",
    "loans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We only consider the following amount of features\n",
    "\n",
    "features = ['grade',              # grade of the loan\n",
    "            'term',               # the term of the loan\n",
    "            'home_ownership',     # home_ownership status: own, mortgage or rent\n",
    "            'emp_length',         # number of years of employment\n",
    "           ]\n",
    "target = 'safe_loans'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract the feature columns and target column\n",
    "loans = loans[features + [target]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122607, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsample dataset to make sure classes are balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('train-idx.json', 'r') as f:\n",
    "    train_idx = json.load(f)\n",
    "with open('validation-idx.json', 'r') as f:\n",
    "    validation_idx = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37224\n",
      "9284\n",
      "46508\n"
     ]
    }
   ],
   "source": [
    "print(len(train_idx))\n",
    "print(len(validation_idx))\n",
    "print(len(train_idx) + len(validation_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding category data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(122607, 5)\n",
      "122607\n"
     ]
    }
   ],
   "source": [
    "loans.head(5)\n",
    "print(loans.shape)\n",
    "print(len(loans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grade', 'term', 'home_ownership', 'emp_length', 'safe_loans']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(loans.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoding_binary_feature(data, y_label):\n",
    "    labels =  data.select_dtypes(include=[object])\n",
    "    encoded_features =[]\n",
    "    #encoded_values = np.transpose(np.array(np.ones(len(data),)))\n",
    "    encoded_values = pd.DataFrame(data[y_label])   \n",
    "    \n",
    "    for label in labels:\n",
    "        \n",
    "        distinct_features = list(set(data[label].values))\n",
    "    \n",
    "        for feature in distinct_features:\n",
    "            encoded_features.append(str(label + '.'+feature))\n",
    "            \n",
    "            #new_array = np.array(np.ones(len(data),))\n",
    "            encoded_values[encoded_features[-1]] = data[label].apply(lambda x : +1 if x==feature else 0)\n",
    "    \n",
    "    return encoded_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['safe_loans' 'grade.A' 'grade.C' 'grade.D' 'grade.G' 'grade.F' 'grade.E'\n",
      " 'grade.B' 'term. 60 months' 'term. 36 months' 'home_ownership.OTHER'\n",
      " 'home_ownership.OWN' 'home_ownership.MORTGAGE' 'home_ownership.RENT'\n",
      " 'emp_length.7 years' 'emp_length.8 years' 'emp_length.1 year'\n",
      " 'emp_length.4 years' 'emp_length.6 years' 'emp_length.9 years'\n",
      " 'emp_length.< 1 year' 'emp_length.5 years' 'emp_length.10+ years'\n",
      " 'emp_length.n/a' 'emp_length.2 years' 'emp_length.3 years']\n"
     ]
    }
   ],
   "source": [
    "loans_data = encoding_binary_feature(loans,'safe_loans')\n",
    "print(loans_data.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "#loans_object = loans.select_dtypes(include=[object])\n",
    "\n",
    "#le = LabelEncoder()\n",
    "\n",
    "#loans_object = loans_object.apply(le.fit_transform)\n",
    "#loans_object.head()\n",
    "\n",
    "#onehotencoder = OneHotEncoder()\n",
    "#X_features = onehotencoder.fit_transform(loans_object).toarray()\n",
    "#print(loans_object.shape)\n",
    "\n",
    "#Y_loans = loans.select_dtypes(exclude=[object])\n",
    "#loans_transform =  pd.concat([loans_left, pd.DataFrame(loans_object)], axis=1,join='inner') \n",
    "#print(Y_loans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(122607, 26)\n"
     ]
    }
   ],
   "source": [
    "print(loans_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "\n",
    "We split the data into a train test split with 80% of the data in the training set and 20% of the data in the test set. We use `seed=1` so that everyone gets the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = loans_data.iloc[train_idx].drop(['safe_loans'], axis = 1)\n",
    "train_Y = loans_data['safe_loans'].iloc[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = loans_data.iloc[validation_idx].drop(['safe_loans'], axis = 1)\n",
    "test_Y = loans_data['safe_loans'].iloc[validation_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safe loan percentage:  50.2236174422\n"
     ]
    }
   ],
   "source": [
    "safe_loans = (train_Y == 1).sum() + (test_Y ==1).sum()\n",
    "bad_loans = (train_Y == -1).sum() + (test_Y == -1).sum()\n",
    "percentage = (safe_loans/(safe_loans + bad_loans)*100)\n",
    "print('Safe loan percentage: ', percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree implementation\n",
    "\n",
    "In this section, we will implement binary decision trees from scratch. There are several steps involved in building a decision tree. For that reason, we have split the entire assignment into several sections.\n",
    "\n",
    "## Function to count number of mistakes while predicting majority class\n",
    "\n",
    "Recall from the lecture that prediction at an intermediate node works by predicting the **majority class** for all data points that belong to this node.\n",
    "\n",
    "Now, we will write a function that calculates the number of **missclassified examples** when predicting the **majority class**. This will be used to help determine which feature is the best to split on at a given node of the tree.\n",
    "\n",
    "**Note**: Keep in mind that in order to compute the number of mistakes for a majority classifier, we only need the label (y values) of the data points in the node. \n",
    "\n",
    "** Steps to follow **:\n",
    "* ** Step 1:** Calculate the number of safe loans and risky loans.\n",
    "* ** Step 2:** Since we are assuming majority class prediction, all the data points that are **not** in the majority class are considered **mistakes**.\n",
    "* ** Step 3:** Return the number of **mistakes**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediate_node_num_mistakes(labels_in_node, key_value):\n",
    "    # Corner case: If labels_in_node is empty, return 0\n",
    "    if len(labels_in_node) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Count the number of 1's (safe loans)\n",
    "    #num_safe_loans = (labels_in_node==1).sum()\n",
    "    \n",
    "    # Count the number of -1's (risky loans)\n",
    "    #num_risky_loans = (labels_in_node ==-1).sum()\n",
    "                \n",
    "    # Return the number of mistakes that the majority classifier makes.\n",
    "    #return num_safe_loans if num_safe_loans < num_risky_loans else num_risky_loans\n",
    "    \n",
    "    if(key_value == -1):\n",
    "        return (labels_in_node==1).sum()\n",
    "    elif key_value == 1:\n",
    "        return (labels_in_node==-1).sum()\n",
    "    else:\n",
    "        num_safe_loans = (labels_in_node==1).sum()\n",
    "    \n",
    "        # Count the number of -1's (risky loans)\n",
    "        num_risky_loans = (labels_in_node ==-1).sum()\n",
    "                \n",
    "        # Return the number of mistakes that the majority classifier makes.\n",
    "        return num_safe_loans if num_safe_loans < num_risky_loans else num_risky_loans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there are several steps in this assignment, we have introduced some stopping points where you can check your code and make sure it is correct before proceeding. To test your `intermediate_node_num_mistakes` function, run the following code until you get a **Test passed!**, then you should proceed. Otherwise, you should spend some time figuring out where things went wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n",
      "Test passed!\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "# Test case 1\n",
    "example_labels = np.array([-1, -1, 1, 1, 1])\n",
    "if intermediate_node_num_mistakes(example_labels, 0) == 2:\n",
    "    print ('Test passed!')\n",
    "else:\n",
    "    print ('Test 1 failed... try again!')\n",
    "\n",
    "# Test case 2\n",
    "example_labels = np.array([-1, -1, 1, 1, 1, 1, 1])\n",
    "if intermediate_node_num_mistakes(example_labels, 0) == 2:\n",
    "    print ('Test passed!')\n",
    "else:\n",
    "    print ('Test 2 failed... try again!')\n",
    "    \n",
    "# Test case 3\n",
    "example_labels = np.array([-1, -1, -1, -1, -1, 1, 1])\n",
    "if intermediate_node_num_mistakes(example_labels, 0) == 2:\n",
    "    print ('Test passed!')\n",
    "else:\n",
    "    print ('Test 3 failed... try again!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to pick best feature to split on\n",
    "\n",
    "The function **best_splitting_feature** takes 3 arguments: \n",
    "1. The data (includes all of the feature columns and label column)\n",
    "2. The features to consider for splits (a list of strings of column names to consider for splits)\n",
    "3. The name of the target/label column (string)\n",
    "\n",
    "The function will loop through the list of possible features, and consider splitting on each of them. It will calculate the classification error of each split and return the feature that had the smallest classification error when split on.\n",
    "\n",
    "Recall that the **classification error** is defined as follows:\n",
    "$$\n",
    "\\mbox{classification error} = \\frac{\\mbox{# mistakes}}{\\mbox{# total examples}}\n",
    "$$\n",
    "\n",
    "Follow these steps: \n",
    "* **Step 1:** Loop over each feature in the feature list\n",
    "* **Step 2:** Within the loop, split the data into two groups: one group where all of the data has feature value 0 or False (we will call this the **left** split), and one group where all of the data has feature value 1 or True (we will call this the **right** split). Make sure the **left** split corresponds with 0 and the **right** split corresponds with 1 to ensure your implementation fits with our implementation of the tree building process.\n",
    "* **Step 3:** Calculate the number of misclassified examples in both groups of data and use the above formula to compute the **classification error**.\n",
    "* **Step 4:** If the computed error is smaller than the best error found so far, store this **feature and its error**.\n",
    "\n",
    "This may seem like a lot, but we have provided pseudocode in the comments in order to help you implement the function correctly.\n",
    "\n",
    "**Note:** Remember that since we are only dealing with binary features, we do not have to consider thresholds for real-valued features. This makes the implementation of this function much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def best_splitting_feature(data, features, target):\n",
    "    \n",
    "    best_feature = None # Keep track of the best feature \n",
    "    best_error = 10     # Keep track of the best error so far \n",
    "    # Note: Since error is always <= 1, we should intialize it with something larger than 1.\n",
    "\n",
    "    # Convert to float to make sure error gets computed correctly.\n",
    "    num_data_points = float(len(target))  \n",
    "    \n",
    "    # Loop through each feature to consider splitting on that feature\n",
    "    for feature in features:\n",
    "        \n",
    "        # The left split will have all data points where the feature value is 0\n",
    "        left_split = data[data[feature] == 0]\n",
    "        \n",
    "        # The right split will have all data points where the feature value is 1\n",
    "        right_split =  data[data[feature] == 1]\n",
    "            \n",
    "        # Calculate the number of misclassified examples in the left split.\n",
    "        # Remember that we implemented a function for this! (It was called intermediate_node_num_mistakes)\n",
    "        left_mistakes =  intermediate_node_num_mistakes(target[data[feature] == 0], -1)           \n",
    "\n",
    "        # Calculate the number of misclassified examples in the right split.\n",
    "        right_mistakes = intermediate_node_num_mistakes(target[data[feature] == 1], 1)      \n",
    "            \n",
    "        # Compute the classification error of this split.\n",
    "        # Error = (# of mistakes (left) + # of mistakes (right)) / (# of data points)\n",
    "        error = (left_mistakes + right_mistakes) / num_data_points\n",
    "        #print(str(error) + feature)\n",
    "        # If this is the best error we have found so far, store the feature as best_feature and the error as best_error\n",
    "        if error <best_error:\n",
    "            best_error = error\n",
    "            best_feature = feature\n",
    "        \n",
    "    \n",
    "    return best_feature # Return the best feature we found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your `best_splitting_feature` function, run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5130, 25)\n",
      "9223\n",
      "28001\n",
      "9412\n"
     ]
    }
   ],
   "source": [
    "print(train_data[train_data['grade.A'] == 1].shape)\n",
    "print((train_data['term. 60 months'] == 1).sum())\n",
    "print((train_data['term. 36 months'] == 1).sum())\n",
    "print((train_data['grade.C'] == 1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['grade.A', 'grade.C', 'grade.D', 'grade.G', 'grade.F', 'grade.E',\n",
       "       'grade.B', 'term. 60 months', 'term. 36 months',\n",
       "       'home_ownership.OTHER', 'home_ownership.OWN',\n",
       "       'home_ownership.MORTGAGE', 'home_ownership.RENT',\n",
       "       'emp_length.7 years', 'emp_length.8 years', 'emp_length.1 year',\n",
       "       'emp_length.4 years', 'emp_length.6 years', 'emp_length.9 years',\n",
       "       'emp_length.< 1 year', 'emp_length.5 years', 'emp_length.10+ years',\n",
       "       'emp_length.n/a', 'emp_length.2 years', 'emp_length.3 years'], dtype=object)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = train_data.columns.values\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1   -1\n",
       "Name: safe_loans, dtype: int64"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grade.A</th>\n",
       "      <th>grade.C</th>\n",
       "      <th>grade.D</th>\n",
       "      <th>grade.G</th>\n",
       "      <th>grade.F</th>\n",
       "      <th>grade.E</th>\n",
       "      <th>grade.B</th>\n",
       "      <th>term. 60 months</th>\n",
       "      <th>term. 36 months</th>\n",
       "      <th>home_ownership.OTHER</th>\n",
       "      <th>...</th>\n",
       "      <th>emp_length.1 year</th>\n",
       "      <th>emp_length.4 years</th>\n",
       "      <th>emp_length.6 years</th>\n",
       "      <th>emp_length.9 years</th>\n",
       "      <th>emp_length.&lt; 1 year</th>\n",
       "      <th>emp_length.5 years</th>\n",
       "      <th>emp_length.10+ years</th>\n",
       "      <th>emp_length.n/a</th>\n",
       "      <th>emp_length.2 years</th>\n",
       "      <th>emp_length.3 years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   grade.A  grade.C  grade.D  grade.G  grade.F  grade.E  grade.B  \\\n",
       "1        0        1        0        0        0        0        0   \n",
       "6        0        0        0        0        1        0        0   \n",
       "\n",
       "   term. 60 months  term. 36 months  home_ownership.OTHER         ...          \\\n",
       "1                1                0                     0         ...           \n",
       "6                1                0                     0         ...           \n",
       "\n",
       "   emp_length.1 year  emp_length.4 years  emp_length.6 years  \\\n",
       "1                  0                   0                   0   \n",
       "6                  0                   1                   0   \n",
       "\n",
       "   emp_length.9 years  emp_length.< 1 year  emp_length.5 years  \\\n",
       "1                   0                    1                   0   \n",
       "6                   0                    0                   0   \n",
       "\n",
       "   emp_length.10+ years  emp_length.n/a  emp_length.2 years  \\\n",
       "1                     0               0                   0   \n",
       "6                     0               0                   0   \n",
       "\n",
       "   emp_length.3 years  \n",
       "1                   0  \n",
       "6                   0  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "returned_feature = best_splitting_feature(train_data, features, train_Y)\n",
    "if  returned_feature == 'term. 36 months':\n",
    "    print ('Test passed!')\n",
    "else:\n",
    "    print ('Test failed... try again!')\n",
    "    print('Returned feature: ', returned_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the tree\n",
    "\n",
    "With the above functions implemented correctly, we are now ready to build our decision tree. Each node in the decision tree is represented as a dictionary which contains the following keys and possible values:\n",
    "\n",
    "    { \n",
    "       'is_leaf'            : True/False.\n",
    "       'prediction'         : Prediction at the leaf node.\n",
    "       'left'               : (dictionary corresponding to the left tree).\n",
    "       'right'              : (dictionary corresponding to the right tree).\n",
    "       'splitting_feature'  : The feature that this node splits on.\n",
    "    }\n",
    "\n",
    "First, we will write a function that creates a leaf node given a set of target values. Fill in the places where you find `## YOUR CODE HERE`. There are **three** places in this function for you to fill in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_leaf(target_values):\n",
    "    \n",
    "    # Create a leaf node\n",
    "    leaf = {'splitting_feature' : None,\n",
    "            'left' : None,\n",
    "            'right' : None,\n",
    "            'is_leaf': True    }   \n",
    "    \n",
    "    # Count the number of data points that are +1 and -1 in this node.\n",
    "    num_ones = len(target_values[target_values == +1])\n",
    "    num_minus_ones = len(target_values[target_values == -1])\n",
    "    \n",
    "    # For the leaf node, set the prediction to be the majority class.\n",
    "    # Store the predicted class (1 or -1) in leaf['prediction']\n",
    "    if num_ones > num_minus_ones:\n",
    "        leaf['prediction'] =          1\n",
    "    else:\n",
    "        leaf['prediction'] =          -1\n",
    "        \n",
    "    # Return the leaf node        \n",
    "    return leaf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have provided a function that learns the decision tree recursively and implements 3 stopping conditions:\n",
    "1. **Stopping condition 1:** All data points in a node are from the same class.\n",
    "2. **Stopping condition 2:** No more features to split on.\n",
    "3. **Additional stopping condition:** In addition to the above two stopping conditions covered in lecture, in this assignment we will also consider a stopping condition based on the **max_depth** of the tree. By not letting the tree grow too deep, we will save computational effort in the learning process. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decision_tree_create(data, features, target, current_depth = 0, max_depth = 10):\n",
    "    \n",
    "    remaining_features = features[:] # Make a copy of the features.\n",
    "    \n",
    "    target_values = target\n",
    "    print (\"--------------------------------------------------------------------\")\n",
    "    print (\"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values)))\n",
    "    \n",
    "\n",
    "    # Stopping condition 1\n",
    "    # (Check if there are mistakes at current node.\n",
    "    # Recall you wrote a function intermediate_node_num_mistakes to compute this.)\n",
    "    if intermediate_node_num_mistakes(target,0) == 0: \n",
    "        print (\"Stopping condition 1 reached.\" )    \n",
    "        # If not mistakes at current node, make current node a leaf node\n",
    "        return create_leaf(target_values)\n",
    "    \n",
    "    # Stopping condition 2 (check if there are remaining features to consider splitting on)\n",
    "    if len(remaining_features) == 0: \n",
    "        print (\"Stopping condition 2 reached.\")    \n",
    "        # If there are no remaining features to consider, make current node a leaf node\n",
    "        return create_leaf(target_values)    \n",
    "    \n",
    "    # Additional stopping condition (limit tree depth)\n",
    "    if current_depth >= max_depth: \n",
    "        print (\"Reached maximum depth. Stopping for now.\")\n",
    "        # If the max tree depth has been reached, make current node a leaf node\n",
    "        return create_leaf(target_values)\n",
    "\n",
    "    # Find the best splitting feature (recall the function best_splitting_feature implemented above)\n",
    "\n",
    "    splitting_feature = best_splitting_feature(data, features, target_values)\n",
    "    # Split on the best feature that we found. \n",
    "    left_split = data[data[splitting_feature] == 0]\n",
    "    left_target = target_values[data[splitting_feature] == 0]\n",
    "    \n",
    "    right_split = data[data[splitting_feature] == 1] \n",
    "    right_target = target_values[data[splitting_feature] == 1]\n",
    "    \n",
    "    #remaining_features =  remaining_features.remove(splitting_feature)\n",
    "    remaining_features = [x for x in remaining_features if x != splitting_feature]\n",
    "    \n",
    "    print (\"Split on feature %s. (%s, %s)\" % (\\\n",
    "                      splitting_feature, len(left_split), len(right_split)))\n",
    "    \n",
    "    # Create a leaf node if the split is \"perfect\"\n",
    "    if len(left_split) == len(data):\n",
    "        print (\"Creating leaf node.\")\n",
    "        return create_leaf(left_target)\n",
    "    if len(right_split) == len(data):\n",
    "        print (\"Creating leaf node.\")\n",
    "        return create_leaf(right_target)\n",
    "\n",
    "        \n",
    "    # Repeat (recurse) on left and right subtrees\n",
    "    left_tree = decision_tree_create(left_split, remaining_features, left_target, current_depth + 1, max_depth)        \n",
    "\n",
    "    right_tree = decision_tree_create(right_split, remaining_features, right_target, current_depth + 1, max_depth) \n",
    "\n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a recursive function to count the nodes in your tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_nodes(tree):\n",
    "    if tree['is_leaf']:\n",
    "        return 1\n",
    "    return 1 + count_nodes(tree['left']) + count_nodes(tree['right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following test code to check your implementation. Make sure you get **'Test passed'** before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (37224 data points).\n",
      "Split on feature term. 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (9223 data points).\n",
      "Split on feature grade.A. (9122, 101)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9122 data points).\n",
      "Split on feature home_ownership.OTHER. (9119, 3)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (9119 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (3 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (101 data points).\n",
      "Split on feature term. 60 months. (0, 101)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (28001 data points).\n",
      "Split on feature grade.A. (22972, 5029)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (22972 data points).\n",
      "Split on feature grade.B. (13654, 9318)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (13654 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (9318 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5029 data points).\n",
      "Split on feature home_ownership.MORTGAGE. (2282, 2747)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (2282 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (2747 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "small_data_decision_tree = decision_tree_create(train_data, features, train_Y, max_depth = 3)\n",
    "\n",
    "if count_nodes(small_data_decision_tree) == 13:\n",
    "    print ('Test passed!')\n",
    "else:\n",
    "    print ('Test failed... try again!')\n",
    "    print ('Number of nodes found                :', count_nodes(small_data_decision_tree))\n",
    "    print ('Number of nodes that should be there : 13' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the tree!\n",
    "\n",
    "Now that all the tests are passing, we will train a tree model on the **train_data**. Limit the depth to 6 (**max_depth = 6**) to make sure the algorithm doesn't run for too long. Call this tree **my_decision_tree**. \n",
    "\n",
    "**Warning**: This code block may take 1-2 minutes to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (37224 data points).\n",
      "Split on feature term. 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (9223 data points).\n",
      "Split on feature grade.A. (9122, 101)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9122 data points).\n",
      "Split on feature home_ownership.OTHER. (9119, 3)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (9119 data points).\n",
      "Split on feature emp_length.n/a. (8898, 221)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (8898 data points).\n",
      "Split on feature grade.B. (7884, 1014)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (7884 data points).\n",
      "Split on feature emp_length.8 years. (7507, 377)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (7507 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (377 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (1014 data points).\n",
      "Split on feature emp_length.5 years. (935, 79)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (935 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (79 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (221 data points).\n",
      "Split on feature grade.G. (215, 6)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (215 data points).\n",
      "Split on feature emp_length.7 years. (215, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (6 data points).\n",
      "Split on feature home_ownership.OWN. (4, 2)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (4 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (2 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (3 data points).\n",
      "Split on feature grade.F. (2, 1)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (2 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (1 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (101 data points).\n",
      "Split on feature term. 60 months. (0, 101)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (28001 data points).\n",
      "Split on feature grade.A. (22972, 5029)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (22972 data points).\n",
      "Split on feature grade.B. (13654, 9318)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (13654 data points).\n",
      "Split on feature term. 60 months. (13654, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (9318 data points).\n",
      "Split on feature home_ownership.MORTGAGE. (4975, 4343)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (4975 data points).\n",
      "Split on feature home_ownership.RENT. (738, 4237)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (738 data points).\n",
      "Split on feature home_ownership.OWN. (17, 721)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (17 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (721 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (4237 data points).\n",
      "Split on feature emp_length.2 years. (3741, 496)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (3741 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (496 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (4343 data points).\n",
      "Split on feature emp_length.10+ years. (2846, 1497)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (2846 data points).\n",
      "Split on feature emp_length.2 years. (2513, 333)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (2513 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (333 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (1497 data points).\n",
      "Split on feature grade.C. (1497, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5029 data points).\n",
      "Split on feature home_ownership.MORTGAGE. (2282, 2747)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (2282 data points).\n",
      "Split on feature home_ownership.RENT. (458, 1824)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (458 data points).\n",
      "Split on feature home_ownership.OWN. (9, 449)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (9 data points).\n",
      "Split on feature home_ownership.OTHER. (0, 9)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (449 data points).\n",
      "Split on feature emp_length.10+ years. (318, 131)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (318 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (131 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (1824 data points).\n",
      "Split on feature emp_length.< 1 year. (1564, 260)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (1564 data points).\n",
      "Split on feature emp_length.2 years. (1332, 232)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (1332 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (232 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (260 data points).\n",
      "Split on feature grade.C. (260, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (2747 data points).\n",
      "Split on feature emp_length.10+ years. (1770, 977)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (1770 data points).\n",
      "Split on feature emp_length.4 years. (1584, 186)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (1584 data points).\n",
      "Split on feature emp_length.2 years. (1380, 204)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (1380 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (204 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (186 data points).\n",
      "Split on feature grade.C. (186, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (977 data points).\n",
      "Split on feature grade.C. (977, 0)\n",
      "Creating leaf node.\n"
     ]
    }
   ],
   "source": [
    "# Make sure to cap the depth at 6 by using max_depth = 6\n",
    "my_decision_tree =decision_tree_create(train_data, features, train_Y, max_depth = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions with a decision tree\n",
    "\n",
    "As discussed in the lecture, we can make predictions from the decision tree with a simple recursive function. Below, we call this function `classify`, which takes in a learned `tree` and a test point `x` to classify.  We include an option `annotate` that describes the prediction path when set to `True`.\n",
    "\n",
    "Fill in the places where you find `## YOUR CODE HERE`. There is **one** place in this function for you to fill in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(tree, x, annotate = False):   \n",
    "    # if the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        if annotate: \n",
    "            print (\"At leaf, predicting %s\" % tree['prediction'])\n",
    "        return tree['prediction'] \n",
    "    else:\n",
    "        # split on feature.\n",
    "        split_feature_value = x[tree['splitting_feature']]\n",
    "        if annotate: \n",
    "            print (\"Split on %s = %s\" % (tree['splitting_feature'], split_feature_value))\n",
    "        if split_feature_value == 0:\n",
    "            return classify(tree['left'], x, annotate)\n",
    "        else:\n",
    "            return classify(tree['right'], x, annotate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's consider the first example of the test set and see what `my_decision_tree` model predicts for this data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grade.A                    0\n",
       "grade.C                    0\n",
       "grade.D                    1\n",
       "grade.G                    0\n",
       "grade.F                    0\n",
       "grade.E                    0\n",
       "grade.B                    0\n",
       "term. 60 months            1\n",
       "term. 36 months            0\n",
       "home_ownership.OTHER       0\n",
       "home_ownership.OWN         0\n",
       "home_ownership.MORTGAGE    0\n",
       "home_ownership.RENT        1\n",
       "emp_length.7 years         0\n",
       "emp_length.8 years         0\n",
       "emp_length.1 year          0\n",
       "emp_length.4 years         0\n",
       "emp_length.6 years         0\n",
       "emp_length.9 years         0\n",
       "emp_length.< 1 year        0\n",
       "emp_length.5 years         0\n",
       "emp_length.10+ years       0\n",
       "emp_length.n/a             0\n",
       "emp_length.2 years         1\n",
       "emp_length.3 years         0\n",
       "Name: 24, dtype: int64"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: -1 \n"
     ]
    }
   ],
   "source": [
    "print ('Predicted class: %s ' % classify(my_decision_tree, test_data.iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating your decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will write a function to evaluate a decision tree by computing the classification error of the tree on the given dataset.\n",
    "\n",
    "Again, recall that the **classification error** is defined as follows:\n",
    "$$\n",
    "\\mbox{classification error} = \\frac{\\mbox{# mistakes}}{\\mbox{# total examples}}\n",
    "$$\n",
    "\n",
    "Now, write a function called `evaluate_classification_error` that takes in as input:\n",
    "1. `tree` (as described above)\n",
    "2. `data` (an SFrame)\n",
    "3. `target` (a string - the name of the target/label column)\n",
    "\n",
    "This function should calculate a prediction (class label) for each row in `data` using the decision `tree` and return the classification error computed using the above formula. Fill in the places where you find `## YOUR CODE HERE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_error(tree, data, target):\n",
    "    # Apply the classify(tree, x) to each row in your data\n",
    "    #prediction = data.apply(lambda x: classify(tree, x))\n",
    "    prediction =[]\n",
    "    for i in range(len(data)):\n",
    "        prediction.append(classify(tree, data.iloc[i]))\n",
    "    \n",
    "    # Once you've made the predictions, calculate the classification error and return it\n",
    "    errors = (prediction == target).sum()/len(target)\n",
    "    return errors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61805256355019389"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(my_decision_tree, test_data, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61538254889318722"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(my_decision_tree, train_data, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question:** Rounded to 2nd decimal point, what is the classification error of **my_decision_tree** on the **test_data**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing out a decision stump\n",
    "\n",
    "As discussed in the lecture, we can print out a single decision stump (printing out the entire tree is left as an exercise to the curious reader). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stump(tree, name = 'root'):\n",
    "    split_name = tree['splitting_feature'] # split_name is something like 'term. 36 months'\n",
    "    if split_name is None:\n",
    "        print (\"(leaf, label: %s)\" % tree['prediction'])\n",
    "        return None\n",
    "    split_feature, split_value = split_name.split('.')\n",
    "    print ('                       %s' % name)\n",
    "    print ('         |---------------|----------------|')\n",
    "    print ('         |                                |')\n",
    "    print ('         |                                |')\n",
    "    print ('         |                                |')\n",
    "    print ('  [{0} == 0]               [{0} == 1]    '.format(split_name))\n",
    "    print ('         |                                |')\n",
    "    print ('         |                                |')\n",
    "    print ('         |                                |')\n",
    "    print ('    (%s)                         (%s)' \\\n",
    "        % (('leaf, label: ' + str(tree['left']['prediction']) if tree['left']['is_leaf'] else 'subtree'),\n",
    "           ('leaf, label: ' + str(tree['right']['prediction']) if tree['right']['is_leaf'] else 'subtree')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [term. 36 months == 0]               [term. 36 months == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (subtree)                         (subtree)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question:** What is the feature that is used for the split at the root node?\n",
    "\n",
    "### Exploring the intermediate left subtree\n",
    "\n",
    "The tree is a recursive dictionary, so we do have access to all the nodes! We can use\n",
    "* `my_decision_tree['left']` to go left\n",
    "* `my_decision_tree['right']` to go right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       term. 36 months\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade.A == 0]               [grade.A == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (subtree)                         (leaf, label: 1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree['left'], my_decision_tree['splitting_feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       term. 36 months\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade.A == 0]               [grade.A == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (subtree)                         (subtree)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree['right'], my_decision_tree['splitting_feature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the left subtree of the left subtree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       grade.A\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [home_ownership.OTHER == 0]               [home_ownership.OTHER == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (subtree)                         (subtree)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree['left']['left'], my_decision_tree['left']['splitting_feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       grade.A\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [emp_length.n/a == 0]               [emp_length.n/a == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (subtree)                         (subtree)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree['left']['left']['left'], my_decision_tree['left']['splitting_feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(leaf, label: 1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree['left']['right'], my_decision_tree['left']['splitting_feature'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
